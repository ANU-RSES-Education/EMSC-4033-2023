# EMSC4033 - Project Report
#### Putri Natari Ratna - u7582024

## Seismic Risk Index Map of Perth, Western Australia Based on Building Vulnerability
---

### **Introduction**

This project comprises three programs designed to facilitate seismic risk analysis and visualization.  

1. **`job_xml_generator.py`**

    `job_xml_generator.py` is a program designed to generate NRML data model (XML-based data) and configuration file that serve as input for `oq_calculation.py`. It accepts user input to enable customization during the file generation process, so that users can provide specific parameters, settings, or configuration details. There are four file that could be created, including `exposure_model.xml`, `fragility_model.xml`, `rupture_model.xml`, and `job.ini`.  
---
2. **`oq_calculation.py`**  

    `oq_calculation.py` is a program aimed to performs OpenQuake calculations required for ground motion modeling and probability of damage distribution. It uses input files generated from `job_xml_generator.py` and a `csv` file that has information about exposure model defined in `exposure_model.xml`.  
---
3. **`streamlit_app.py`**  

    `streamlit_app.py` is a program that offer a streamlined dashboard experience on Streamlit, allowing users to explore and interact with the outputs generated by `oq_calculation.py` alongside predefined `shapefile` (\*.shp) data.  

Together, these programs provide a comprehensive solution for seismic risk assessment and visualization, simplifying the process of analyzing and visualizing potential earthquake impacts.

---
### **Instructions**

To successfully run this program, please follow these instructions:

1. **Install Dependencies**: Here are some dependencies that should be installed to run the program:
    - `openquake.engine`: a module related to the OpenQuake engine, which is an open-source platform for seismic hazard and risk assessment. It provides functionalities for performing seismic calculations and analysis.
    - `streamlit`: a Python library used for creating interactive and customizable web applications for data science and machine learning projects.
    - `numpy`: a fundamental library for working with large, multi-dimensional arrays and matrices, along with a wide range of mathematical functions to operate on these arrays efficiently.
    - `scipy`: a library that provides additional scientific and numerical computing tools, including modules for optimization, interpolation, linear algebra, statistics, signal processing, and more.
    - `pandas`: a library for data manipulation and analysis. It provides data structures, such as dataframes, and functions for handling and processing structured data.
    - `geopandas`: an extension of the pandas library that adds support for working with geospatial data. It provides data structures to handle geospatial datasets, along with functions for spatial operations, mapping, and visualization
    - `matplotlib`: a plotting library for creating various types of static, animated, and interactive visualizations. 
    - `folium`: a library used for creating interactive maps and visualizations. It is built on top of the leaflet.js library and allows users to generate maps with markers, polygons, heatmaps, and other overlays. Folium is often used for geospatial visualizations in web applications.
    - `streamlit-folium`: a library that integrates two great open-source projects in the Python ecosystem: Streamlit and Folium
    - `branca`: a library that provides utilities for creating and customizing interactive leaflet.js maps. It offers functions for creating map tiles, controlling map appearance, adding overlays, and incorporating other elements into leaflet-based maps.
    - `beautifulsoup4`: a library used for web scraping and parsing HTML or XML documents. 
    - `lxml` : a library that contains xml parser for `beautifulsoup4`.  


    All of these dependencies is provided in `requirements.txt`. To install it, open the terminal or command prompt and navigate to your working directory. Once inside the directrory, execute the following command:
    ```bash
    pip install -r requirements.txt
    ```
    This command will automatically install all the dependencies listed in the requirements.txt file, ensuring that the system has all the necessary packages and libraries to run the code smoothly.
    
    > **WARNING**
    > - streamlit version required in this program only works for `Python` version >= 3.7 and !=3.9.7  
    > - If you already have some of libraries/modules defined in `requirements.txt`, there may be some dependency conflicts, and you may need to resintall them by executing the following command:
    > ``` js
    > pip install --upgrade --force-reinstall -r requirements.txt
    > ```
---
2. **Run `job_xml_generator.py`**
    - Prepare all parameters/setting that will be inputted in this program. As a sample, `sample_parameters.xlsx` is prepared in `samples` folder as an example to guide you running this program.
    - Create `data` folder.
    - Prepare `csv` file that has information about exposure model that will be defined in `exposure_model.xml`. This file should be stored in `data` folder. Information about how to create the `csv` file could be accessed in [OpenQuake_docs](https://docs.openquake.org/oq-engine/manual/latest/risk.html). As a sample, `new_exposure_model.csv` is also prepared in `samples\data` folder.
    - To run this program execute the following command in your terminal or command prompt:  
    ```bash
    python job_xml_generator.py
    ```
    - After you execute the program, a user prompt will appear where you can choose which file you want to generate first. Once you chose, there will be another prompt that asks you to fill all of the parameters. You can use parameters in `sample_parameters.xlsx` in `samples` folder here.
    - All generated files (`exposure_model.xml`, `fragility_model.xml`, `rupture_model.xml`, and `job.ini`) will be stored in `data` folder.
    - As a sample, `exposure_model.xml`, `fragility_model.xml`, `rupture_model.xml`, and `job.ini` have been generated and could be found in `samples\data` folder.
---
3. **Run `oq_calculation.py`**
    - Input files for this program are generated from `job_xml_generator.py`.
    - Make sure that all `xml` files defined in `job.ini` has the same name with `xml` file in the folder.
    - Run this program by executing the following command:
    ``` js
    python oq_calculation.py
    ```
    - There are 5 outputs generated by this program, including `gmf_data_X.csv`,`avg_gmf_X.csv`,`damages-rlzs-000_X.csv`, `aggrisk-_X.csv`, and `risk_by_event_X.csv`, with `X` is `calculation_id`. For example, if one of your output is `avg_gmf_160.csv`, your `calculation_id` is `160`. These files are stored in `output` folder. 
    - As a sample, gmf_data_095.csv`,`avg_gmf_095.csv`,`damages-rlzs-000_095.csv`, `aggrisk.csv`, and `risk_by_event.csv` have been generated and could be found in `samples\output` folder. 
---
4. **Run `streamlit_app.py`**
    - Input files for this program are generated from  `oq_calculation.py` program and predefined `shapefile` data required for spatial visualization. These data could be found in `output` and `SHP` folder, respectively.
    - To run this program, execute the following command:
    ``` js
    streamlit run streamlit_app.py -- [calculation_id]
    ```
    - As described before, `calculation_id` is id for every output from `oq_calculation.py`
    - This command will start the Streamlit server, and you can access the dashboard by opening the provided local URL in your web browser.

---

### **Testing and Validation**
#### **Validation**
There are several validation mechanisms to ensure the validity of user inputs. These validations are crucial to guarantee that the provided inputs adhere to the expected format and meet specific criteria. Here is a brief explanation of the validation process:
1. **Numeric Validation**: To validate numeric inputs, the program check if the user-provided values are indeed numbers, whether it is `int` or `float`.If the input is not a valid number, an error message is displayed, prompting the user to provide a valid numeric value. This validation is implemented in `checkNumber(message,int_num=False,lower_lim=None,upper_lim=None)` function. 
2. **Input Range Validation**: For inputs that have specific ranges or limits, such as numerical values within a certain range, the program validate whether the provided input falls within the acceptable range.This validation is also implemented in `checkNumber(message,int_num=False,lower_lim=None,upper_lim=None)` function. 
3. **String Validation**: It involves verifying that user inputs are in the expected string format. This may include checking for the presence of certain characters or validating against predefined patterns or regular expressions. This validation implemented in some function, e.g. `text_input(message,allow_spaces=False)` function. 
---
#### **Testing**
In this project, **unit testing** is used to validate the functionality of the `job_xml_generator` module. There are several types of testing performed in this code:

1. **Function Testing**: Several individual functions within the `job_xml_generator` module are tested using the `unittest.TestCase` class. Each test method is named descriptively to indicate the specific function being tested. For example, `test_cost_type_pick()` tests the `cost_type_pick()` function.
2. **Input Testing**: The `unittest.mock.patch` decorator is used to simulate user input during the test cases. By patching the `builtins.input` function, you can control the inputs provided to the functions being tested. The `side_effect` parameter of patch is set to a list of input values to simulate a sequence of user inputs.
3. **Assertion Testing**: The `assertEqual()` method from the `unittest.TestCase` class is used to compare the actual results of the tested functions against the expected results. This assertion ensures that the functions produce the desired output.
4. **File Testing**: The `test_create_job_ini()` method tests the `create_job_ini()` function, which generates a `job.ini` file. This test case mocks the file writing process using `builtins.open` and `mock_open`, allowing you to capture the written content and verify if it matches the expected content.

The code for these testing is `tests.py`. To run this test, execute the following command:
```bash
python -m unittest tests
```

---

### **Samples and Demo**
Samples of the output generated from `job_xml_generator.py` and `oq_calculation.py` could be found in `samples` folder. These samples could be used to run streamlit dashboard demo. `SHP` folder that holds all shapefile needed to run the dashboard is already available. To run the demo, execute the following command:

```bash
streamlit run demo_streamlit_app.py
```
This command will start the Streamlit server, and you can access the dashboard by opening the provided local URL in your web browser. 

An example pdf-version of the streamlit dashboard generated from `demo_streamlit_app.py` can be seen in [this pdf file](./streamlit_app.pdf) 

---
### **Limitations**

There are some limitations of these programs,
1. `job_xml_generator.py`
    - `rupture_model_input()` and `create_rupture_model_xml()` functions only work for **Simple Fault Geometry**. It has not facilitated other rupture types like **Arbitrary Fault Rupture**, **Single Planar Rupture OR Multi-Planar Rupture**, and **Complex Fault Rupture**.
    - `job_ini_input()` and `create_job_ini()` functions are designed to create configuration file for **scenario_damage** calculation mode only. It cannot be used for other modes that `OpenQuake Engine` facilitate.
2. `oq_calculation.py`  
    This program also only works for **scenario_damage** calculation mode.
3. `streamlit_app.py`  
    This program loading of substantial data to generate charts, which consequently requires a long time to create the dashboard. Additionally, when users interact with the dashboard and initiate changes, there is a tendency for the program to reload and restart the chart plotting process from the beginning. Consequently, this reloading process can result in extended waiting periods for users.

---

### **Future Improvements**	
To address the limitations mentioned, here are some potential future improvements:
1. Extend the functionality of `job_xml_generator.py` to support additional rupture types, such as **Arbitrary Fault Rupture, Single Planar Rupture, Multi-Planar Rupture, and Complex Fault Rupture**. This would provide more flexibility for users in defining the desired rupture characteristics.
2. Modify `job_xml_generator.py` and `oq_calculation.py` to support calculation modes beyond just **scenario_damage**.
3. Separate processes of data loading and transformation from generating dashboard to reduce the time it takes to create the dashboard. One possible solution for this is to use `pickle` library that provides functionality for serializing (pickling) and deserializing (unpickling) Python objects.
